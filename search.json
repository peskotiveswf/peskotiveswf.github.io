[{"authors":["Chia-Hsuan Lee","Hung-Yi Lee","Szu-Lin Wu","Chi-Liang Liu","Wei Fang","Jui-Yang Hsu","Bo-Hsiang Tseng"],"categories":null,"content":"","date":1556337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556337600,"objectID":"9dc4f78d9498b4cc5a042f9257c80be6","permalink":"https://people.csail.mit.edu/weifang/publication/taslp19-qa/","publishdate":"2019-04-27T00:00:00-04:00","relpermalink":"/weifang/publication/taslp19-qa/","section":"publication","summary":"Although multimedia or spoken content presents more attractive information than plain text content, the former is more difficult to display on a screen and be selected by a user. As a result, for humans, accessing large collections of spoken content is much more difficult and time-consuming than doing so for text content. It would therefore be helpful to develop machines which understand spoken content. In this paper, we propose two new tasks for machine comprehension of spoken content. The first is a listening comprehension test for TOEFL, a challenging academic English examination for English learners whose native languages are not English. We show that the proposed model outperforms the naive approaches and other neural network based models by exploiting the hierarchical structures of natural languages and the selective power of attention mechanism. For the second listening comprehension task – spoken SQuAD – we find that speech recognition errors severely impair machine comprehension; we propose the use of subword units to mitigate the impact of these errors.","tags":[],"title":"Machine Comprehension of Spoken Content: TOEFL Listening Test and Spoken SQuAD","type":"publication"},{"authors":["Moin Nadeem","Wei Fang","Brian Xu","Mitra Mohtarami","James Glass"],"categories":null,"content":"","date":1554091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554091200,"objectID":"443aa1c61bd875984b6bca90c122b68a","permalink":"https://people.csail.mit.edu/weifang/publication/naacl19-faktademo/","publishdate":"2019-04-01T00:00:00-04:00","relpermalink":"/weifang/publication/naacl19-faktademo/","section":"publication","summary":"We present FAKTA which is a unified framework that integrates various components of a fact checking process: document retrieval from media sources with various types of reliability, stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality of given claims and provides evidence at the document and sentence level to explain its predictions.","tags":[],"title":"FAKTA: An Automatic End-to-End Fact Checking System","type":"publication"},{"authors":["Chung-Wei Lee","Wei Fang","Chih-Kuan Yeh","Yu-Chiang Frank Wang"],"categories":null,"content":"","date":1529294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529294400,"objectID":"7cb725c1a496898e77f21bbca64e32d1","permalink":"https://people.csail.mit.edu/weifang/publication/cvpr18-mlzsl/","publishdate":"2018-06-18T00:00:00-04:00","relpermalink":"/weifang/publication/cvpr18-mlzsl/","section":"publication","summary":"In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance. Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels. Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels. With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks. Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method.","tags":[],"title":"Multi-Label Zero-Shot Learning with Structured Knowledge Graphs","type":"publication"},{"authors":null,"categories":null,"content":"Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a novel deep learning framework that incorporates knowledge graphs for describing the relationships between multiple labels to achieve multi-label zero-shot learning (ML-ZSL).\n","date":1525147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525147200,"objectID":"c287156946b1ea153bd66c38d3959521","permalink":"https://people.csail.mit.edu/weifang/project/vll18-mlzsl/","publishdate":"2018-05-01T00:00:00-04:00","relpermalink":"/weifang/project/vll18-mlzsl/","section":"project","summary":"Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a novel deep learning framework that incorporates knowledge graphs for describing the relationships between multiple labels to achieve multi-label zero-shot learning (ML-ZSL). ","tags":["computer-vision","deep-learning"],"title":"Multi-Label Zero-Shot Recognition with Knowledge Graphs","type":"project"},{"authors":null,"categories":null,"content":"","date":1512547267,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512547267,"objectID":"3b9aab79ece00dd76e6f31c7869bd79e","permalink":"https://people.csail.mit.edu/weifang/activities/ntueesa/","publishdate":"2017-12-06T16:01:07+08:00","relpermalink":"/weifang/activities/ntueesa/","section":"activities","summary":"","tags":[],"title":"NTUEE Student Association","type":"activities"},{"authors":null,"categories":null,"content":"","date":1506744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506744000,"objectID":"60270818a58deed4f1d180afc91defa4","permalink":"https://people.csail.mit.edu/weifang/project/spml17-audio2vec/","publishdate":"2017-09-30T00:00:00-04:00","relpermalink":"/weifang/project/spml17-audio2vec/","section":"project","summary":"","tags":["speech-processing","deep-learning"],"title":"Audio Word Embeddings","type":"project"},{"authors":null,"categories":null,"content":"In this project, we built a modular dialogue system with deep learning techniques for the task of assisting people in department stores.\nHere is our poster.\n","date":1498017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498017600,"objectID":"d5aebb69f8657b9f620f5e4d058502b2","permalink":"https://people.csail.mit.edu/weifang/project/icb17-chatbot/","publishdate":"2017-06-21T00:00:00-04:00","relpermalink":"/weifang/project/icb17-chatbot/","section":"project","summary":"We built a modular dialogue system with deep learning techniques for the task of assisting people in department stores.","tags":["natural-language-processing","deep-learning"],"title":"Modular Chatbot","type":"project"},{"authors":["Wei Fang","Jui-Yang Hsu","Hung-Yi Lee","Lin-Shan Lee"],"categories":null,"content":"","date":1481605200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481605200,"objectID":"e1d9d9183569299e647714b01168d503","permalink":"https://people.csail.mit.edu/weifang/publication/slt16-toeflqa/","publishdate":"2016-12-13T00:00:00-05:00","relpermalink":"/weifang/publication/slt16-toeflqa/","section":"publication","summary":"Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.","tags":[],"title":"Hierarchical Attention Model for Improved Machine Comprehension of Spoken Content","type":"publication"},{"authors":null,"categories":null,"content":"Here is a short talk I gave on our group\u0026rsquo;s recent work on speech question answering.\n","date":1481605200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481605200,"objectID":"b25204c6aee5255124eaf25f1fda6a2a","permalink":"https://people.csail.mit.edu/weifang/project/spml16-toeflqa/","publishdate":"2016-12-13T00:00:00-05:00","relpermalink":"/weifang/project/spml16-toeflqa/","section":"project","summary":"Here is a short talk I gave on our group\u0026rsquo;s recent work on speech question answering.","tags":["natural-language-processing","speech-processing","deep-learning"],"title":"Question Answering for Listening Comprehension","type":"project"},{"authors":null,"categories":null,"content":"In the 2016 HackNTU Hackathon, our team developed MasterView, a virtual reality system that shows the expert\u0026rsquo;s moves in first-person perspective so the user can imitate and practice.\nHere\u0026rsquo;s a demo video of our system, and here\u0026rsquo;s our presentation slides in Mandarin Chinese.\n","date":1471752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471752000,"objectID":"d8e0b09f4ee3dde59068c7d5a000a01b","permalink":"https://people.csail.mit.edu/weifang/project/hackntu16-masterview/","publishdate":"2016-08-21T00:00:00-04:00","relpermalink":"/weifang/project/hackntu16-masterview/","section":"project","summary":"At the 2016 HackNTU Hackathon, our team developed MasterView, a virtual reality system that shows the expert's moves in first-person perspective so the user can imitate and practice.","tags":["hackathon","virtual-reality"],"title":"MasterView (VR System)","type":"project"},{"authors":null,"categories":null,"content":"Using common and well-known development tools, we transform slither.io, a game based on \u0026ldquo;Snake\u0026rdquo;, into an interactive augmented-reality game.\nThe details about this project can be found in our project report.\n","date":1466481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466481600,"objectID":"3838447251089f4aef5c946891476dfb","permalink":"https://people.csail.mit.edu/weifang/project/cg16-slither/","publishdate":"2016-06-21T00:00:00-04:00","relpermalink":"/weifang/project/cg16-slither/","section":"project","summary":"Using common and well-known development tools, we transform slither.io, a game based on 'Snake', into an interactive augmented-reality game.","tags":["augmented-reality","game","unity-3d"],"title":"Slither.AR (AR Game)","type":"project"},{"authors":null,"categories":null,"content":"We participated in the 2015 CAD Contest hosted by IEEE/ACM International Conference on Computer Aided Design (ICCAD 2015). In Problem B, we developed a system to identify equivalent cuts in both equivalent and nonequivalent circuits for design partitioning.\n","date":1442635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442635200,"objectID":"65885c3b974ce8ea4bc4c9c1daf397d5","permalink":"https://people.csail.mit.edu/weifang/project/cad15-eqcheck/","publishdate":"2015-09-19T00:00:00-04:00","relpermalink":"/weifang/project/cad15-eqcheck/","section":"project","summary":"In the 2015 CAD Contest, we developed a system to identify equivalent cuts in both equivalent and nonequivalent circuits for design partitioning.","tags":["contest","computer-aided-design","electronic-design-automation"],"title":"Large Scale Equivalence Checking and Functional Correction","type":"project"},{"authors":null,"categories":null,"content":"At the 2016 HackNTU Hackathon, our team built a second-hand trading platform, uhu, which provided services for booking, notifications, and filtering functions based on Facebook trading groups, in under 42 hours. Our presentation slides (in Mandarin Chinese) can be found here.\n","date":1440129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440129600,"objectID":"8f70a230e5b3fbd9d2f6529cac7b517d","permalink":"https://people.csail.mit.edu/weifang/project/hackntu15-uhu/","publishdate":"2015-08-21T00:00:00-04:00","relpermalink":"/weifang/project/hackntu15-uhu/","section":"project","summary":"At the 2016 HackNTU Hackathon, our team built a second-hand trading platform, uhu, which provided services for booking, notifications, and filtering functions based on Facebook trading groups, in under 42 hours.","tags":["hackathon","web"],"title":"uHu - a Resale Service","type":"project"}]